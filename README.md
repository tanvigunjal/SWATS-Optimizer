# SWATS-Optimizer
SWATS optimizer (switching from Adam to SGD)

# Overview
Adaptive optimization methods like RMSprop, Adagrad or Adam generalizes poorly despite their superior training performance when compared to Stochastic gradient descent (SGD). Adaptive optimization methods perform well in the initial training stage but lack in performance in later stages due to unstable and non-uniform learning rate at the end of training. Hence, SGD generalizes beter when compared to adaptive methods.



